{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transform(image):\n",
    "    choice = random.randint(0, 5)\n",
    "\n",
    "    rows, cols, _ = image.shape\n",
    "\n",
    "    if choice == 0: # Translation\n",
    "        tx, ty = random.randint(-20, 20), random.randint(-20, 20)\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        return cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "    elif choice == 1: # Rotation\n",
    "        angle = random.randint(-30, 30)\n",
    "        M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
    "        return cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "    elif choice == 2: # Scaling\n",
    "        scale_factor = random.uniform(0.8, 1.2)\n",
    "        return cv2.resize(image, None, fx=scale_factor, fy=scale_factor)\n",
    "\n",
    "    elif choice == 3: # Skewing\n",
    "        pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "        pt1 = 5+10*np.random.uniform()-10/2\n",
    "        pt2 = 20+10*np.random.uniform()-10/2\n",
    "        pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "        M = cv2.getAffineTransform(pts1,pts2)\n",
    "        return cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "    elif choice == 4: # Perspective distortion\n",
    "        pts1 = np.float32([[5,5],[20,5],[5,20],[20,20]])\n",
    "        pts2 = np.float32([[0,0],[20,5],[5,20],[20,20]])\n",
    "        M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "        return cv2.warpPerspective(image, M, (cols, rows))\n",
    "\n",
    "    else: # Pincushion distortion\n",
    "        distortion = np.float32([1, 1, 0, 0])\n",
    "        camera_matrix = np.eye(3)\n",
    "        return cv2.undistort(image, camera_matrix, distortion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images size:  (15000, 28, 28)  and val_images size:  (45000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# # creating validation images val_images\n",
    "# val_size = int(len(train_labels)*0.25)\n",
    "# train_images , val_images= train_images[:val_size], train_images[val_size:]\n",
    "# print(\"train_images size: \", train_images.shape, \" and val_images size: \", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "new_shape = (32, 32)\n",
    "train_images = np.array([cv2.resize(img, new_shape) for img in train_images])\n",
    "test_images = np.array([cv2.resize(img, new_shape) for img in test_images])\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "train_images = train_images[..., tf.newaxis].astype(\"float32\")\n",
    "test_images = test_images[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ViT model\n",
    "input_shape = (*new_shape, 1)\n",
    "patch_size = 4\n",
    "num_heads = 2\n",
    "num_patches = (new_shape[0] // patch_size) * (new_shape[1] // patch_size)\n",
    "projection_dim = 32\n",
    "num_transformer_layers = 2\n",
    "mlp_head_units = [64, 32]\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "x = tf.keras.layers.Conv2D(3, (3, 3), padding=\"same\")(inputs)  # Convert to 3 channels\n",
    "x = tf.keras.layers.Rescaling(1.0 / 255)(x)  # Rescale\n",
    "x = tf.keras.layers.Conv2D(32, (patch_size, patch_size), strides=patch_size)(x)\n",
    "x = tf.keras.layers.Reshape((num_patches, 32))(x)\n",
    "\n",
    "for _ in range(num_transformer_layers):\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "    )(x1, x1)\n",
    "    x2 = tf.keras.layers.Add()([attention_output, x])\n",
    "    x3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    x3 = tf.keras.layers.Conv1D(64, 2, padding=\"same\", activation=\"relu\")(x3)\n",
    "    x3 = tf.keras.layers.Dropout(0.1)(x3)\n",
    "    x3 = tf.keras.layers.Conv1D(32, 2, padding=\"same\")(x3)\n",
    "    x = tf.keras.layers.Add()([x3, x2])\n",
    "\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "for dim in mlp_head_units:\n",
    "    x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 164s 82ms/step - loss: 2.3035 - accuracy: 0.1097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b872b7bdc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "# Fit the model with early stopping\n",
    "history = model.fit(X_train, y_train, validation_data = (X_val,yval), verbose = 1,epochs = 200 ,batch_size = 20, callbacks=[early_stopping])\n",
    "history = model.fit(train_images, train_labels, epochs=1, batch_size=32, batch_size = 20, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the performance on taining_data\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(history.history['loss']) # tb\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and report as before...\n",
    "\n",
    "# Applying transformations to test images\n",
    "# Applying transformations to test images\n",
    "# Applying transformations to test images\n",
    "# Applying transformations to test images\n",
    "transformed_test_images = []\n",
    "for img in test_images:\n",
    "    transformed_img = random_transform(img)\n",
    "    transformed_test_images.append(transformed_img)\n",
    "# resizing transformed_test_images into (32, 32)\n",
    "new_shape = (32, 32)\n",
    "transformed_test_images = np.array([cv2.resize(img, new_shape) for img in transformed_test_images])\n",
    "\n",
    "# Checking for shape consistency\n",
    "first_shape = transformed_test_images[0].shape\n",
    "for i, img in enumerate(transformed_test_images):\n",
    "    if img.shape != first_shape:\n",
    "        print(f\"Inconsistent shape at index {i}: expected {first_shape}, but got {img.shape}\")\n",
    "\n",
    "# Add more diagnostic code if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 29ms/step - loss: 2.3011 - accuracy: 0.1135\n",
      "313/313 [==============================] - 10s 28ms/step\n",
      "Test Accuracy: 0.11349999904632568\n",
      "Precision: 0.01135\n",
      "Recall: 0.1\n",
      "F1 Score: 0.0203861697350696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salah\\.conda\\envs\\vision-transformer\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "test_loss, test_acc = model.evaluate(transformed_test_images, test_labels)\n",
    "\n",
    "# Getting Precision, Recall, and F1 Score\n",
    "y_pred = model.predict(transformed_test_images)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(test_labels, y_pred_labels, average='macro')\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 30ms/step - loss: 2.3011 - accuracy: 0.1135\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "Test Accuracy: 0.11349999904632568\n",
      "Precision: 0.01135\n",
      "Recall: 0.1\n",
      "F1 Score: 0.0203861697350696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salah\\.conda\\envs\\vision-transformer\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# # Evaluating the model\n",
    "# test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "# # Getting Precision, Recall, and F1 Score\n",
    "# y_pred = model.predict(test_images)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# precision, recall, f1_score, _ = precision_recall_fscore_support(test_labels, y_pred_labels, average='macro')\n",
    "\n",
    "# print(f\"Test Accuracy: {test_acc}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
